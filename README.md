# Overview

 Using Apache Spark on Cloud Dataproc to distribute a computationally intensive image processing task onto a cluster of machines. Tasks done include:
 - Creating a managed Cloud Dataproc cluster with Apache Spark.
 - Building and running jobs that use external packages not readily available in the cluster.
 - Shutting down the cluster.
 
 
# Introduction

## Step 1:  Create a development machine in Compute Engine
