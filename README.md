# Overview

 Using Apache Spark on Cloud Dataproc to distribute a computationally intensive image processing task onto a cluster of machines. Tasks done include:
 - Creating a managed Cloud Dataproc cluster with Apache Spark.
 - Building and running jobs that use external packages not readily available in the cluster.
 - Shutting down the cluster.
 
 
# Introduction

...

# Process

### Step 1:  Create a development machine in Compute Engine

...

### Step 2:  Install external image processing software

...

### Step 3: Create a Cloud Storage bucket and collect images

...

### Step 4: Create a Cloud Dataproc cluster

...

### Step 5: Submit the job to Cloud Dataproc

...
